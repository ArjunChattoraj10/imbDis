SM4$auc = auc(SM4)
SM4$auc
library(caret)
simMetric = function(orig.labels, pred.labels, pred.probs,
case, bins = seq(0.1,0.5,0.1)){
# check values
param_check(orig.labels, pred.labels, pred.probs, case, bins)
# initialize the class via list object
val = list(orig.labels = orig.labels, pred.labels = pred.labels,
pred.probs = pred.probs, bins = bins, case = case)
attr(val, "class") = "simMetric" # sets the class
return(val)
}
orig = rbinom(n, 1, 0.2)
pred = rbinom(n, 1, 0.2)
orig
pred
table(orig, pred)
class(pred.probs)
typeof(pred.probs)
standardizeLabels = function(labels, case){
# converts a labels vector to
labels_01 = rep(0, length(labels))
labels_01[labels == case] = 1
}
standardiseLabels = standardizeLabels
standardiseLabels()
standardiseLabels
orig
pred
table(orig, pred)
# pivot table
# rows for original, columns for predicted
pivot = table(orig, pred)
pivot[0,0]
pivot[0,0]
pivot
pivot[0,0]
pivot[1,1]
pivot
pivot[1,2]
pivot[2,1]
pivot
sum(pred)
sum(orig)
180+44
pivot
bins
rep(0, len(bins))
rep(0, length(bins))
?sample
sample(5,3)
sample(5,3, T)
sample(5,3, T)
samp_size
# define S3 class for new metric
library(pROC)
simMetric = function(orig.labels, pred.labels, pred.probs,
case, bins = seq(0.1,0.5,0.1)){
# check values
param_check(orig.labels, pred.labels, pred.probs, case, bins)
# initialize the class via list object using provided arguments
obj = list(orig.labels = orig.labels, pred.labels = pred.labels,
pred.probs = pred.probs, bins = bins, case = case)
attr(obj, "class") = "simMetric" # sets the class
# define control and add to list
obj$control = unique(pred.labels[pred.labels != case])
# define standardized labels and add to list
obj$orig_01 = standardizeLabels(orig.labels, case)
obj$pred_01 = standardizeLabels(pred.labels, case)
# define sample size and bin case sizes
obj$sample.size = calc_samplesize(obj$pred_01, bins)
obj$bin.caseSizes = round(bins*obj$sample.size)
return(val)
}
param_check = function(orig.labels, pred.labels, pred.probs, case, bins){
# function to check if arguments of a simMetric object are valid
if(length(unique(orig.labels)) != 2)
stop('\'orig.labels\' may only have 2 distinct values.')
if(class(orig.probs) != "numeric")
stop('\'orig.probs\' contains non-numeric values.')
if(length(unique(pred.labels)) != 2)
stop('\'pred.labels\' may only have 2 distinct values.')
if(class(pred.probs) != "numeric")
stop('\'pred.probs\' contains non-numeric values.')
if(length(orig.labels) != length(pred.labels))
stop('Length of \'orig.labels\' does not match length of \'pred.labels\'.')
if(any(pred.probs < 0 | pred.probs > 1))
stop('At least one element of \'pred.probs\' is less than 0 or greater than 1.')
if(length(pred.labels) != length(pred.probs))
stop('Length of \'pred.labels\' does not match length of \'pred.probs\'.')
if(!case %in% labels)
stop('Value of \'case\' is not present in \'labels\'.')
if(class(bins) != "numeric")
stop('\'bins\' is non-numeric.')
if(any(bins < 0 | bins > 1))
stop('At least one element of \'bins\' is less than 0 or greater than 1.')
if(any(bins == 0 | bins == 1))
stop('Elements of \'bin\' cannot be exactly 0 or 1.')
}
standardizeLabels = function(labels, case){
# converts a binary vector of labels  to a vector
# where all case values are 1 and controls are 0
labels_01 = rep(0, length(labels))
labels_01[labels == case] = 1
return(labels_01)
}
standardiseLabels = standardizeLabels # alternate spelling
calc_samplesize = function(labels_01, bins){
## total cases and controls in labels
tot_cases = sum(labels_01)
tot_controls = sum(labels_01 == 0)
## proportion of cases and controls in labels
prop_cases = mean(labels_01)
prop_control = 1 - prop_cases
## sample size calculations
samp_size = round(ifelse(prop_cases <= max(bins),    # if the prop(cases) < max(bins)
tot_cases/max(bins),        # use the cases procedure
tot_controls/max(1-bins)))  # else use the controls procedure
return(samp_size)
}
auc = function(obj) UseMethod("auc")
auc.simMetric = function(obj){
# function takes in an object of class simMetric
# calculates the C-Statistic for specified frequency bins
# define variables to reduce code clutter
pred.probs = obj$pred.probs
bins = obj$bins
labels_01 = obj$pred_01 # necessary for proper functionality of roc()
samp_size = obj$sample.size
cases_list = obj$bin.ncase
## subset labels and probs into only case and only not case
### indices with case label
### ensures correspondence of labels and probabilities after subsetting
case_i = which(labels_01 == 1)
### subset labels and probs for case
labels_case = labels_01[case_i]
probs_case = pred.probs[case_i]
### subset labels and prons for not case
labels_control = labels_01[-case_i]
probs_control = pred.probs[-case_i]
## compute returned values
auroc = rep(0, length(bins))
n_samps = rep(0, length(bins))
for(i in 1:length(bins)){
# set up ROC calculations
n_cases = cases_list[i]
n_control = samp_size - n_cases
# random sample of indices for case and non-case labels and probabilities
sub_i_case = sample(length(probs_case), n_cases)
sub_i_control = sample(length(probs_control), n_control)
# labels and probabilities organized as all elements corresponding to case
# followed by all elements corresponding to control
labs = c(labels_case[sub_i_case], labels_control[sub_i_control])
probs = c(probs_case[sub_i_case], probs_control[sub_i_control])
# shuffle labels and predictions
# same indices used to subset ensures correspondence of
# labels and predictions after shuffling
i_test = sample(length(labs))
labs = labs[i_test]
probs = probs[i_test]
# calculate AUC and save results
roc_i = roc(labs, probs) # default control = 0, case = 1
auroc[i] = roc_i$auc
n_samps[i] = length(probs)
}
# return data frame
return(data.frame(bins, auroc, n_samps))
}
f1 = function(obj) UseMethod("f1")
f1.simMetric = function(obj){
# standardized labels only
orig = obj$orig_01
pred = obj$pred_01
bins = obj$bins
samp_size = obj$sample.size
cases_list = obj$bin.caseSizes
## subset labels into only case and only not case
### indices with predicted case label
### ensures correspondence of labels after subsetting
case_i = which(pred == 1)
### subset labels for case
orig_case = orig[case_i]
pred_case = pred[case_i]
### subset labels for not case
orig_control = orig[-case_i]
pred_control = pred[-case_i]
## compute returned values
f1 = rep(0, length(bins))
n_samps = rep(0, length(bins))
for(i in 1:length(bins)){
# set up ROC calculations
n_cases = cases_list[i]
n_control = samp_size - n_cases
# random sample of indices for case and non-case labels and probabilities
sub_i_case = sample(length(orig_case), n_cases)
sub_i_control = sample(length(orig_control), n_control)
# labels and probabilities organized as all elements corresponding to case
# followed by all elements corresponding to control
orig_sub = c(orig_case[sub_i_case], orig_control[sub_i_control])
pred_sub = c(pred_case[sub_i_case], pred_control[sub_i_control])
# random shuffle not necessary as confusion matrix
# does not depend on order of entries
# perform F1 calculation
# pivot table -- confusion matrix for subsetted data
# rows for original, columns for predicted
pivot = table(orig_sub, pred_sub)
# trueNeg = pivot[1,1]
truePos = pivot[2,2]
# falseNeg = pivot[2,1]
# falsePos = pivot[1,2]
totOrigPos = sum(orig_sub)
totPredPos = sum(pred_sub)
# precision and recall
prec = truePos/totPredPos
rec = truePos/totOrigPos
# use F1 formula and save to vector
f1[i] = 2*pred*rec/(prec+rec)
n_samps[i] = length(pred_sub)
}
# return data frame
return(data.frame(bins, f1, n_samps))
}
source("class_def.R")
## to ensure calls work
SM1 = simMetric(c(1,1,1,0), c(1,1,0,1), c(0.1,0.2,0.3,0.4), 1, c(0.1,0.2,0.3,0.4))
source("class_def.R")
## to ensure calls work
SM1 = simMetric(c(1,1,1,0), c(1,1,0,1), c(0.1,0.2,0.3,0.4), 1, c(0.1,0.2,0.3,0.4))
## to ensure calls work
SM1 = simMetric(c(1,1,1,0), c(1,1,0,1), c(0.1,0.2,0.3,0.4), 1, c(0.1,0.2,0.3,0.4))
source("class_def.R")
## to ensure calls work
SM1 = simMetric(c(1,1,1,0), c(1,1,0,1), c(0.1,0.2,0.3,0.4), 1, c(0.1,0.2,0.3,0.4))
SM2 = simMetric(orig.labels = c(1,1,1,0),pred.labels = c(1,1,0,1),
pred.probs = c(0.1,0.2,0.3,0.4), case = 1)
## real function calls
set.seed(34)
n = 1000
orig.labels = rbinom(n, 1, 0.2)
pred.labels = rbinom(n, 1, 0.2)
# make realistic pred.probs
pred.probs= rep(0, n)
for(i in 1:n){
pred.probs[i] = ifelse(pred.labels[i] == 1,
rnorm(1,0.75, 0.2),
rnorm(1,0.25, 0.2))
}
pred.probs[pred.probs < 0] = 0.05
pred.probs[pred.probs > 1] = 0.95
case = 1
SM3 = simMetric(orig.labels, pred.labels, pred.probs, case)
orig.labels
SM3 = simMetric(orig.labels, pred.labels, pred.probs, case)
SM1
orig.labels
class(orig.labels)
class(pred.labels)
is.numeric(orig.labels)
is.numeric(orig.labels)
!is.numeric(orig.labels)
source("class_def.R")
## to ensure call works
SM1 = simMetric(c(1,1,1,0), c(1,1,0,1), c(0.1,0.2,0.3,0.4), 1, c(0.1,0.2,0.3,0.4))
SM2 = simMetric(orig.labels = c(1,1,1,0),pred.labels = c(1,1,0,1),
pred.probs = c(0.1,0.2,0.3,0.4), case = 1)
set.seed(34)
n = 1000
orig.labels = rbinom(n, 1, 0.2)
pred.labels = rbinom(n, 1, 0.2)
# make realistic pred.probs
pred.probs= rep(0, n)
for(i in 1:n){
pred.probs[i] = ifelse(pred.labels[i] == 1,
rnorm(1,0.75, 0.2),
rnorm(1,0.25, 0.2))
}
pred.probs[pred.probs < 0] = 0.05
pred.probs[pred.probs > 1] = 0.95
case = 1
SM3 = simMetric(orig.labels, pred.labels, pred.probs, case)
SM3$auc = auc(SM3)
SM3$sample.size
class(SM3$sample.size)
SM3$f1 = f1(SM3)
source("class_def.R")
# Testing for simMetric
## to ensure call works
SM1 = simMetric(c(1,1,1,0), c(1,1,0,1), c(0.1,0.2,0.3,0.4), 1, c(0.1,0.2,0.3,0.4))
SM2 = simMetric(orig.labels = c(1,1,1,0),pred.labels = c(1,1,0,1),
pred.probs = c(0.1,0.2,0.3,0.4), case = 1)
## real function calls
set.seed(34)
n = 1000
orig.labels = rbinom(n, 1, 0.2)
pred.labels = rbinom(n, 1, 0.2)
# make realistic pred.probs
pred.probs= rep(0, n)
for(i in 1:n){
pred.probs[i] = ifelse(pred.labels[i] == 1,
rnorm(1,0.75, 0.2),
rnorm(1,0.25, 0.2))
}
pred.probs[pred.probs < 0] = 0.05
pred.probs[pred.probs > 1] = 0.95
case = 1
SM3 = simMetric(orig.labels, pred.labels, pred.probs, case)
SM3$f1 = f1(SM3)
SM3$f1
SM3$auc = auc(SM3)
SM3$auc = auc(SM3)
SM3$auc = auc(SM3)
labels_01
labels_01 = SM3$pred_01
labels_01
### indices with case label
### ensures correspondence of labels and probabilities after subsetting
case_i = which(labels_01 == 1)
case_i
### subset labels and probs for case
labels_case = labels_01[case_i]
labels_case
probs_case = pred.probs[case_i]
probs_case
### subset labels and prons for not case
labels_control = labels_01[-case_i]
probs_control = pred.probs[-case_i]
labels_control
probs_control
## compute returned values
auroc = rep(0, length(bins))
n_samps = rep(0, length(bins))
auroc
n_samps
# set up ROC calculations
n_cases = cases_list[i]
source("class_def.R")
# Testing for simMetric
## to ensure call works
SM1 = simMetric(c(1,1,1,0), c(1,1,0,1), c(0.1,0.2,0.3,0.4), 1, c(0.1,0.2,0.3,0.4))
SM2 = simMetric(orig.labels = c(1,1,1,0),pred.labels = c(1,1,0,1),
pred.probs = c(0.1,0.2,0.3,0.4), case = 1)
## real function calls
set.seed(34)
n = 1000
orig.labels = rbinom(n, 1, 0.2)
pred.labels = rbinom(n, 1, 0.2)
# make realistic pred.probs
pred.probs= rep(0, n)
for(i in 1:n){
pred.probs[i] = ifelse(pred.labels[i] == 1,
rnorm(1,0.75, 0.2),
rnorm(1,0.25, 0.2))
}
pred.probs[pred.probs < 0] = 0.05
pred.probs[pred.probs > 1] = 0.95
case = 1
SM3 = simMetric(orig.labels, pred.labels, pred.probs, case)
SM3$auc = auc(SM3)
SM3$auc
bins = seq(0.05,0.2,0.05)
SM4 = simMetric(orig.labels, pred.labels, pred.probs, case, bins)
SM4$auc = auc(SM4)
SM4$f1 = f1(SM4)
SM4$f1
SM4$auc
# Random Forest Classification
# Ensure presence of file dependencies
source("train_test.R")
library(caret)
set.seed(225)
tc = trainControl(method = "cv", number = )
rf = train(y ~ ., data = train_dat, method = "rf",
trControl = trainControl(method = "cv"))
# Scoring metrics
predsRF_01 = predict(rf$finalModel, test_dat)
# Confusion Matrix
conmatRF = confusionMatrix(predsRF_01, test_dat$y); conmatRF
## Brier Score
## F1 score
precisionRF = conmatRF$byClass["Pos Pred Value"]
recallRF = conmatRF$byClass["Sensitivity"]
F1_RF = 2*precisionRF*recallRF/(precisionRF + recallRF); as.numeric(F1_RF)
## C-statistic
# Ensure presence of file dependencies
source("train_test.R")
plot(sort(predict(lr1,type = "link")))
1- logLik(lr1)/logLik(null_lr)
# ensure presence of file dependencies
source("train_test.R")
# Ensure presence of file dependencies
source("train_test.R")
getwd()
sim_dat = read.csv("../data/simulated_data_arjun.csv")
# decision tree with all predictors
dec_tree = train(y ~ ., data = train_dat, method = "rpart")
library(caret)
library(rattle)
library(rpart)
# decision tree with all predictors
dec_tree = train(y ~ ., data = train_dat, method = "rpart")
# Ensure presence of file dependencies
source("train_test.R") # Performs the train/test split
# Ensure presence of file dependencies
source("train_test.R") # Performs the train/test split
# Ensure presence of file dependencies
source("train_test.R") # Performs the train/test split
# Ensure presence of file dependencies
source("train_test.R") # Performs the train/test split
source("class_def.R")  # Loads the class into the environment
library(caret)
library(rattle)
library(rpart)
# decision tree with all predictors
dec_tree = train(y ~ ., data = train_dat, method = "rpart")
fancyRpartPlot(dec_tree$finalModel, palettes = c("Reds","Greens"))
# decision tree with all predictors
dec_tree = train(factor(y) ~ ., data = train_dat, method = "rpart")
fancyRpartPlot(dec_tree$finalModel, palettes = c("Reds","Greens"))
# decision tree with all predictors
dec_tree = train(y ~ ., data = train_dat, method = "rpart")
fancyRpartPlot(dec_tree$finalModel, palettes = c("Reds","Greens"))
# decision tree with all predictors
dec_tree = train(factor(y) ~ ., data = train_dat, method = "rpart")
fancyRpartPlot(dec_tree$finalModel, palettes = c("Reds","Greens"))
prune.control = rpart.control(xval = 10)
dec2 = rpart(y ~ ., data = train_dat, method = "class", control = prune.control)
fancyRpartPlot(dec2, palettes = c("Reds","Greens"))
# decision tree with all predictors
dec_tree_all = train(factor(y) ~ ., data = train_dat, method = "rpart")
fancyRpartPlot(dec_tree_all$finalModel, palettes = c("Reds","Greens"))
# decision tree with 7 predictors
dec_tree_7 = train(factor(y) ~ x1+x2+x3+x4+x5+x6+x7, data = train_dat, method = "rpart")
fancyRpartPlot(dec_tree_7$finalModel, palettes = c("Reds","Greens"))
# decision tree with all predictors
dec_tree_all = train(factor(y) ~ ., data = train_dat, method = "rpart")
fancyRpartPlot(dec_tree_all$finalModel, palettes = c("Reds","Greens"))
# decision tree with 7 predictors
dec_tree_7 = train(factor(y) ~ x1+x2+x3+x4+x5+x6+x7, data = train_dat, method = "rpart")
fancyRpartPlot(dec_tree_7$finalModel, palettes = c("Reds","Greens"))
# obtain predicted probabilities and labels
predict(dec_tree_all$finalModel, test_dat)
train_dat
head(train_dat)
head(predict(dec_tree_all$finalModel, test_dat))
# obtain predicted probabilities and labels
predict(dec_tree_all$finalModel, test_dat)
predict(dec_tree_7$finalModel, test_dat)
# obtain predicted probabilities and labels
predict(dec_tree_all$finalModel, test_dat)
predict(dec_tree_7$finalModel, test_dat)
# obtain predicted probabilities and labels
preds_DT_all = predict(dec_tree_all$finalModel, test_dat)
preds_DT_7 = predict(dec_tree_7$finalModel, test_dat)
preds_DT_7
predict(dec_tree_7$finalModel, test_dat)
head(preds_DT_7)
head(test_dat)
# decision tree with all predictors
dec_tree_all = train(factor(y, levels = c(1,0)) ~ ., data = train_dat, method = "rpart")
fancyRpartPlot(dec_tree_all$finalModel, palettes = c("Reds","Greens"))
# obtain predicted probabilities and labels
preds_DT_all = predict(dec_tree_all$finalModel, test_dat)
head(preds_DT_all)
# decision tree with all predictors
dec_tree_all = train(factor(y) ~ ., data = train_dat, method = "rpart")
fancyRpartPlot(dec_tree_all$finalModel, palettes = c("Reds","Greens"))
# decision tree with 7 predictors
dec_tree_7 = train(factor(y) ~ x1+x2+x3+x4+x5+x6+x7, data = train_dat, method = "rpart")
fancyRpartPlot(dec_tree_7$finalModel, palettes = c("Reds","Greens"))
# obtain predicted probabilities and labels
preds_DT_all = predict(dec_tree_all$finalModel, test_dat)
preds_DT_7 = predict(dec_tree_7$finalModel, test_dat)
preds_DT_all > 0.7, 1, 0
ifelse(preds_DT_all > 0.7, 1, 0)
preds_DT_all[,1]
preds_DT_all = predict(dec_tree_all$finalModel, test_dat)
labs_DT_all = ifelse(preds_DT_all[,1] > 0.5, 0, 1)
preds_DT_7 = predict(dec_tree_7$finalModel, test_dat)
labs_DT_7 = ifelse(preds_DT_7[,1] > 0.5, 0, 1)
# Define class - DT all predictors
SM_DT_all = simMetric(test_dat, labs_DT_all, preds_DT_all, 1)
# Define class - DT all predictors
SM_DT_all = simMetric(test_dat$y, labs_DT_all, preds_DT_all, 1)
labs_DT_all
preds_DT_all[,2]
# Define class - DT all predictors
SM_DT_all = simMetric(test_dat$y, labs_DT_all, preds_DT_all[,2], 1)
preds_DT_all[,2]
labs_DT_all
test_dat$y
labs_DT_all
# Define class - DT all predictors
SM_DT_all = simMetric(test_dat$y, labs_DT_all, preds_DT_all[,2], 1)
source("class_def.R")  # Loads the class into the environment
# Define class - DT all predictors
SM_DT_all = simMetric(test_dat$y, labs_DT_all, preds_DT_all[,2], 1)
auc.simMetric(SM_DT_all)
f1.simMetric(SM_DT_all)
auc.simMetric(SM_DT_all)
f1.simMetric(SM_DT_all)
SM_DT_7 = simMetric(test_dat$y, labs_DT_7, preds_DT_7[,2], 1)
SM_DT_7 = simMetric(test_dat$y, labs_DT_7, preds_DT_7[,2], 1)
# c-statistic and F1 for 7 preds DT
auc.simMetric(SM_DT_7)
f1.simMetric(SM_DT_7)
tc = trainControl(method = "cv", number = 10)
rf = train(y ~ ., data = train_dat, method = "rf",
trControl = trainControl(method = "cv"))
rf$preds
# tc = trainControl(method = "cv", number = 10)
rf = train(y ~ ., data = train_dat, method = "rf")
